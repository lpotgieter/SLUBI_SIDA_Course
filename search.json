[
  {
    "objectID": "Day4Session1_results.html",
    "href": "Day4Session1_results.html",
    "title": "RNASeq Results",
    "section": "",
    "text": "In this session, we will have a look at some of the results that were produced with the nf core pipelines we ran. It’s all good and well running analyses, but you need to be able to do something with the output.",
    "crumbs": [
      "Home",
      "Hands-on training",
      "RNASeq Results"
    ]
  },
  {
    "objectID": "Day2Session2_nextflow_handson.html",
    "href": "Day2Session2_nextflow_handson.html",
    "title": "nfcore RNASeq Pipeline",
    "section": "",
    "text": "Now that you know about Linux, containers, pixi, and Nextflow, we get to start with the really cool part of our course! In this section, we will create a pixi environment containing nf-core and nextflow. Once we’ve done that, we will turn our attention to nf-core to set up the rnaseq pipeline. Finally, we will run the pipeline.",
    "crumbs": [
      "Home",
      "Hands-on training",
      "nfcore RNASeq Pipeline"
    ]
  },
  {
    "objectID": "Day2Session2_nextflow_handson.html#introduction",
    "href": "Day2Session2_nextflow_handson.html#introduction",
    "title": "nfcore RNASeq Pipeline",
    "section": "",
    "text": "Now that you know about Linux, containers, pixi, and Nextflow, we get to start with the really cool part of our course! In this section, we will create a pixi environment containing nf-core and nextflow. Once we’ve done that, we will turn our attention to nf-core to set up the rnaseq pipeline. Finally, we will run the pipeline.",
    "crumbs": [
      "Home",
      "Hands-on training",
      "nfcore RNASeq Pipeline"
    ]
  },
  {
    "objectID": "Day2Session2_nextflow_handson.html#setting-up-your-pixi-environment",
    "href": "Day2Session2_nextflow_handson.html#setting-up-your-pixi-environment",
    "title": "nfcore RNASeq Pipeline",
    "section": "Setting Up Your Pixi Environment",
    "text": "Setting Up Your Pixi Environment\nIn our course directory execute these commands, one after the other.\nLet’s inititalise an environment for this. Again, please substitute your name in the name part of the commands.\npixi init name_nextflow -c conda-forge -c bioconda\nChange directory into the project you created, and just list the files there\ncd name_nextflow\nls\nAdd nf-core and Nextflow\npixi add nextflow nf-core\n\n\n\n\n\n\nNote\n\n\n\n\n\nWhile apptainer is sticky loaded on this server, it won’t always be the case for other servers. So, you can add apptainer in the add command\n\n\n\nAnd just check that everything worked, summon the help message from nf-core\npixi run nf-core --help",
    "crumbs": [
      "Home",
      "Hands-on training",
      "nfcore RNASeq Pipeline"
    ]
  },
  {
    "objectID": "Day2Session2_nextflow_handson.html#preparing-the-run",
    "href": "Day2Session2_nextflow_handson.html#preparing-the-run",
    "title": "nfcore RNASeq Pipeline",
    "section": "Preparing The Run",
    "text": "Preparing The Run\nFrom the nf-core homepage, we can search for pipelines. We are going to demo the rnaseq pipeline. You can see that there is a lot going on in this pipeline! We will chat more about these things in class rather than including screenshots of everything.\n\n\n\nrnaseq landing page\n\n\nUnder the Usage, you will find a lot of information that describes the pipeline, including input information, the samplesheet.csv (this one we will make together in class).\nUnder the Parameters tab you will find information on all of the things that we will set up in the next step.\nUnder the Output tab, you will find information on the expected output generated from the pipeline. This is useful to help you interpret what the pipeline produces.\nTo set up our own analysis, we will click on the launch version 3.18.0 button. (This was the version on the website at the time of writing this session. The version number may change). We are then redirected to a page where we can fill in all of our information about input files, as well as selecting or deselecting certain parts of the pipeline. We will share the things here that you need to input each time, and go through some finer details based on the discussion with you.\n\nSetting working and results directories\n\n\n\n\n\n\nImportant\n\n\n\nWe recommend that you use absolute paths rather than relative paths for setting your runs up\n\n\nDuring the first part, you need to set a working and result directory. If you are using a server that has a profile established, you can put the name of the server there. If not, we will create our own configuration profile if we run into memory issues.\n\n\n\nSetting work and output directories\n\n\n\n\n\nSetting results and input CSV\n\n\nWe will compile the input CSV together in class. This is entirely unique to each analysis.\n\n\n\n\n\n\nTip\n\n\n\n\n\nTo list all fastq files with their absolute path, one per line\nfind . -maxdepth 1 -type f -name \"*.fastq.gz\" -exec realpath {} \\;\nYou can substitute the . that indicates it’s only looking in the directory you’re currently in to any other path on your file system\n\n\n\n\nConfiguration profiles\nSince we are working on a server with a configuration profile established, we have downloaded it and put it in the course folder. If you want to fetch it for yourself\nwget https://raw.githubusercontent.com/hpc2n/intro-course/master/exercises/NEXTFLOW/INTERACTIVE/hpc2n.config\nHere is the configuration profile on HPC2N from the above link. The most important things we need to pay attention to are the max_memory, max_cpus, and max_time settings. If you want to create your own profile, you can adjust these to suit your system requirements.\n// Config profile for HPC2N\nparams {\n  config_profile_description = 'Cluster profile for HPC2N'\n  config_profile_contact = 'Pedro Ojeda @pojeda'\n  config_profile_url = 'https://www.hpc2n.umu.se/'\n  project = null\n  clusterOptions = null\n  max_memory = 128.GB\n  max_cpus = 28\n  max_time = 168.h\n  email = 'pedroojeda2011@gmail.com'\n}\n\nsingularity {\n  enabled = true\n}\n\nprocess {\n  executor = 'slurm'\n  clusterOptions = { \"-A $params.project ${params.clusterOptions ?: ''}\" }\n}\n\n\n\n\n\n\nNote\n\n\n\nIf you copy this config file for yourself, you need to remove the process section, unless you have a slurm job manager installed on your cluster. If you have this, you will defiinitely have a system administrator who can help you write this block to suit your system!\n\n\n\n\n\nSetting all other inputs that are required\nIn this section, you set variables that are related to your reference genome. If you are using something listed on iGenomes, you can input that name. If you are working with your own reference genome, or something not listed, you need to input the absolute path of the reference genomes you have downloaded.\n\n\n\nReference genome options\n\n\nDepending on your strategy, you might need to input a corresponding gff as well. It really depends on the kind of analysis you are hoping to perform.\n\n\nObtaining your JSON file\nOnce everything is filled in, click on Launch and you will be redirected to another page containing your JSON file that has information on your run. You can either run the analyses by copying the command at the top of the page (BUT DON’T PRESS ENTER JUST YET) or by copying the JSON file a bit lower on the screen and saving it as nf-params.json in your folder on HPC2N.",
    "crumbs": [
      "Home",
      "Hands-on training",
      "nfcore RNASeq Pipeline"
    ]
  },
  {
    "objectID": "Day2Session2_nextflow_handson.html#starting-the-run",
    "href": "Day2Session2_nextflow_handson.html#starting-the-run",
    "title": "nfcore RNASeq Pipeline",
    "section": "Starting The Run",
    "text": "Starting The Run\nSince we are running this through Pixi to show you the versatility of environments, we need to submit this through a batch script to slurm. Copy the following text to file called name_submit_rnaseq.sh where name is your name.\n#!/bin/bash -l\n#SBATCH -A our_proj_allocation\n#SBATCH -n 5\n#SBATCH -t 24:00:00\n\n/your_home_directory/.pixi/bin/pixi run nextflow run nf-core/rnaseq -r 3.18.0 -params-file /your_path/nf-params.json\nAnd then submit it to slurm with\nsbatch name_submit_rnaseq.sh\n\n\n\n\n\n\nNote\n\n\n\nslurm is a job management tool installed on many servers to distribute resources evenly among users. If you are running this on your own machine later, you can just run it through pixi run as before\n\n\nYou can check the progress of your job with squeue -u your_username\nAnd now we wait until the run is done!",
    "crumbs": [
      "Home",
      "Hands-on training",
      "nfcore RNASeq Pipeline"
    ]
  },
  {
    "objectID": "Day4Session3_r_in_quarto.html",
    "href": "Day4Session3_r_in_quarto.html",
    "title": "Using R in Quarto",
    "section": "",
    "text": "As you’ve seen, Quarto is a really neat tool to integrate different programming languages, aggregate analyses, and write reports. One of the most commonly used languages in bioinformatics is R, and we wanted to show you some neat tips and tricks that we use often to make things a bit easier. Of course, please have a look at the R for Data Science book for more practical tips.\nWhat we are showing you here works just as well with R Markdown, so if you are already using that, or would prefer to work in the R GUI, please feel free to do so.",
    "crumbs": [
      "Home",
      "Hands-on training",
      "Using R in Quarto"
    ]
  },
  {
    "objectID": "Day4Session3_r_in_quarto.html#introduction",
    "href": "Day4Session3_r_in_quarto.html#introduction",
    "title": "Using R in Quarto",
    "section": "",
    "text": "As you’ve seen, Quarto is a really neat tool to integrate different programming languages, aggregate analyses, and write reports. One of the most commonly used languages in bioinformatics is R, and we wanted to show you some neat tips and tricks that we use often to make things a bit easier. Of course, please have a look at the R for Data Science book for more practical tips.\nWhat we are showing you here works just as well with R Markdown, so if you are already using that, or would prefer to work in the R GUI, please feel free to do so.",
    "crumbs": [
      "Home",
      "Hands-on training",
      "Using R in Quarto"
    ]
  },
  {
    "objectID": "Day4Session3_r_in_quarto.html#code-in-r",
    "href": "Day4Session3_r_in_quarto.html#code-in-r",
    "title": "Using R in Quarto",
    "section": "Code in R",
    "text": "Code in R\nIn R, you have the option of working in the console, in an R script or in an R Markdown (among many options). Within the console, you execute code bit by bit. Within an R script you can execute code line-by-line, in sections, or as a whole. In R Markdown, you create a chunk that you can also execute line-by-line or as a whole block. In Quarto, you will use chunks as well.\nA chunk is a bit of code surrounded by backticks. A single backtick highlights things like this. A chunk is started with 3 backticks, filled with code, and closed with 3 backticks. You also indicate the language you will code in with curly brackets and the language you want to use.\n\n\n\n\n\n\nTip\n\n\n\nIf you add a fullstop in front of the language, your code is shown but it is not executed {r} would run an R chunk, but {.r} would not\n\n\n\nSuppressing warnings\nR produces a lot of warnings! Sometimes these warning are useful, but most of the time, they aren’t useful for us. If we look back at yesterday’s example of running R in Quarto:\n\npacman::p_load(tidyverse, palmerpenguins)\n\nggplot(\n  data = penguins,\n  mapping = aes(x = flipper_length_mm, y = body_mass_g)\n) +\n  geom_point(mapping = aes(color = species, shape = species)) +\n  geom_smooth(method = \"lm\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nThere are 2 warnings about removing 2 rows. There are also some warnings about the libraries.\nTo remove these for a whole document, you can add this to the title section of your page to apply only to the page it is on (the title would be your title, this title is just this page’s title):\ntitle: \"Using R in Quarto\"\nwarning: false\n---\nYou can also silence warnings for particular blocks (because sometimes you might care about the warnings!) In that case, you would add this in the first line of your chunk:\n#| warning: false\n\npacman::p_load(tidyverse, palmerpenguins)\n\nggplot(\n  data = penguins,\n  mapping = aes(x = flipper_length_mm, y = body_mass_g)\n) +\n  geom_point(mapping = aes(color = species, shape = species)) +\n  geom_smooth(method = \"lm\")",
    "crumbs": [
      "Home",
      "Hands-on training",
      "Using R in Quarto"
    ]
  },
  {
    "objectID": "Day4Session3_r_in_quarto.html#useful-packages-we-love",
    "href": "Day4Session3_r_in_quarto.html#useful-packages-we-love",
    "title": "Using R in Quarto",
    "section": "Useful Packages We Love!",
    "text": "Useful Packages We Love!\nThere are plenty of really cool packages out there, but the 2 that stand out to us to help with project management and organisation, and keep your environment easy to use are pacman and here\n\npacman\npacman is a package manager that checks whether you have a library installed before loading it with the p_load function that you can see in the chunks above. If you don’t have the package installed, it checks several repositories, installs it, and loads it. If you have it installed, it simply loads it. You can install particular versions with it, install GitHub packages, unload particular packages, as well as temporarily installing packages. This has made life with R a lot easier, especially if you are supporting users/students across multiple versions of R!\n\n\nhere\nhere is a smoother way of setting working directories in R. It sets the working directory relative to a file that is pointed to with the here::i_am(\"file\") command. This is incredibly useful when you are sharing projects and scripts between different people. As long as you are sharing the whole folder and don’t change the architecture (like renaming files and folders), the script will work the same on each system- no more manually changing the setwd() command!",
    "crumbs": [
      "Home",
      "Hands-on training",
      "Using R in Quarto"
    ]
  },
  {
    "objectID": "Day5Session1_AI.html",
    "href": "Day5Session1_AI.html",
    "title": "Caution: AI in Bioinformatics",
    "section": "",
    "text": "In the past 2 years, AI has become more mainstream. The primary kind of AI people think of, is a Large Language Model (LLM). At this point, you cannot really avoid contact with these models anymore as they have infiltrated every aspect of the internet.\n\n\n\nSearching LLM with Google to be answered with an LLM response\n\n\nThese algorithms are language models trained on incredibly large datasets. They are trained to recognise and generate natural language. The chatbots we are all familiar with are generative pretrained transformers (GPTs). These can be trained for specific tasks, or guided by prompt generation.\nThis is not an AI theory course, and we really do not have enough knowledge to explain the underlying theory beyond a rudimentary level. We would like to discuss the impact of AI on you as a user, and advocate for responsible use of AI in your current and future work.",
    "crumbs": [
      "Home",
      "Background knowledge with a bit of hands-on",
      "Caution: AI in Bioinformatics"
    ]
  },
  {
    "objectID": "Day5Session1_AI.html#introduction",
    "href": "Day5Session1_AI.html#introduction",
    "title": "Caution: AI in Bioinformatics",
    "section": "",
    "text": "In the past 2 years, AI has become more mainstream. The primary kind of AI people think of, is a Large Language Model (LLM). At this point, you cannot really avoid contact with these models anymore as they have infiltrated every aspect of the internet.\n\n\n\nSearching LLM with Google to be answered with an LLM response\n\n\nThese algorithms are language models trained on incredibly large datasets. They are trained to recognise and generate natural language. The chatbots we are all familiar with are generative pretrained transformers (GPTs). These can be trained for specific tasks, or guided by prompt generation.\nThis is not an AI theory course, and we really do not have enough knowledge to explain the underlying theory beyond a rudimentary level. We would like to discuss the impact of AI on you as a user, and advocate for responsible use of AI in your current and future work.",
    "crumbs": [
      "Home",
      "Background knowledge with a bit of hands-on",
      "Caution: AI in Bioinformatics"
    ]
  },
  {
    "objectID": "Day5Session1_AI.html#machine-learning-in-bioinformatics",
    "href": "Day5Session1_AI.html#machine-learning-in-bioinformatics",
    "title": "Caution: AI in Bioinformatics",
    "section": "Machine Learning in Bioinformatics",
    "text": "Machine Learning in Bioinformatics\nMachine learning (ML) has been used in bioinformatics for many decades on every level. Before ML, algorithms had to be programmed by hand rather than having the algorithms learn features of a dataset. With ML, features of a dataset can be annotated based on previously annotated datasets. These algorithms were a mix of supervised (learning on annotated data) and unsupervised (learning on unannotated data) learning, depending on the function of the algorithm.\nSupervised algorithms are used for classification and regression analyses. Unsupervised algorithms are used to discover hidden patterns in data without needing a human’s input. Unsupervised algorithms are used in clustering, association, and dimensionality reduction\n\n\n\n\n\n\nNote\n\n\n\n\n\nClassification: Output is a discrete variable. Linear classifiers, support vector machines, decision trees, random forests. E.g. annotating a new genome based on genome annotations from existing species.\nRegression: Focus on understanding dependent and indepedent variables\nClustering:\nAssociation:\nDimensionality reduction:\n\n\n\nFor more info, see here, here, .\nThere is no arguing that these algorithms have led to great progress within the field of bioinformatics",
    "crumbs": [
      "Home",
      "Background knowledge with a bit of hands-on",
      "Caution: AI in Bioinformatics"
    ]
  },
  {
    "objectID": "Day5Session1_AI.html#impact-of-ai",
    "href": "Day5Session1_AI.html#impact-of-ai",
    "title": "Caution: AI in Bioinformatics",
    "section": "Impact of AI",
    "text": "Impact of AI\nChatGPT gained 100 million users in the 2 months after its release in 2022, making it the fastest-growing consumr application in history. Generative AI (GenAI) models now come in many different flavours, depending on the developer.\n\n\n\nGenAI chatbots by market share in August 2025 from FirstPageSage\n\n\n\nShortcomings We Must Be Aware Of\n\nTraining Data\nGenAI has all been trained on existing data. There has always been a bias in whose information gets captured. In a historical context, history was recorded by the party that won the war and destroyed existing knowledge. This also changed as different empires and narratives rose and fell. With digitisation, this information has landed on the internet. In the more modern “Internet Age”, everyone with an internet connection can technically post anything they’d like on the internet. There are different biases at play- not everyone has equal access to the internet, some people may not have strong enough opinions to post about something online, some people prefer to read rather than contribute, while others take pleasure in “shit-posting”.\nThe differences in how people use the internet have an effect on how useful the trained models become. In this example, you can clearly see what the effect of shit-posting is.\n\n\n\nGenAI answer to a simple question\n\n\nGenAI’s are not programmed to say that they do not know something, and will happily hallucinate an answer. If you do not know better, or trust the computers, you may take this answer as true and post it elsewhere. As the use of AI’s increases, AI generated content is used to train new AI’s.\nIn a perfect world, GenAI would be trained on perfectly curated data, but even with all of the data that we have on the internet at the moment, we do not have nearly enough data for this. We have to make do with what we have.",
    "crumbs": [
      "Home",
      "Background knowledge with a bit of hands-on",
      "Caution: AI in Bioinformatics"
    ]
  },
  {
    "objectID": "Day5Session1_AI.html#some-negatives-of-using-ai",
    "href": "Day5Session1_AI.html#some-negatives-of-using-ai",
    "title": "Caution: AI in Bioinformatics",
    "section": "Some Negatives of Using AI",
    "text": "Some Negatives of Using AI\nWe have all seen the amazing things we can do with GenAI, so I will not go into detail about how cool AI is. I will highlight some of the drawbacks of AI, and things we need to keep an eye on as the use of AI increases. I don’t believe we will ever be able to get away from GenAI, but we can make decisions to use AI responsibly.\n\n\n\n\n\n\nCaution\n\n\n\nWhile the lure of AI is becoming more present in our daily lives, remember that you do not need it. You were perfectly able to design a packing list for your upcoming trip. You were able to look at a paper to find answers to your scientific questions. You knew how to query a vignette in R to determine how a function could be implemented.\nLife was a bit slower, but you used your mind and your agency. You made decisions. Please do not confuse convenience with need!\n\n\n\nLearning with AI\nAI has great potential in the field of education. ChatGPT has been shown to be highly beneficial in an educational environment when integrated properly. However, the use of AI in this setting must be balanced and carefully curated. A 2025 pre-print by Kosmyna et al showed that adults that used ChatGPT to write SAT type essays were outperformed consistently by adults that wrote the same essay without the support of an AI, and had significantly lower brain engagement.\n\n\nProductivity with AI\nA recent study by a non-profit group, Model Evaluation and Threat Research [(METR)https://metr.org/] aimed to quantify the difference in productivity when using AI. Participants in this study were not new to their field, with at least 5 years of experience prior to this study being conducted.\n\n\n\nAI reducing productivity\n\n\nThe study also found that when AI is allowed, the participants spent less time coding and seeking solutions to the problems. Rather, they spent time prompting the AI, reading and reviewing responses, and being idle. Intel produced similar findings.\n\n\n\nReasons for loss of productivity with AI\n\n\nIf one combines the effect of passively learning with AI and the loss of productivity with, it becomes clear that AI must be used very carefully\n\n\nData Privacy\n\n\nGlobal Linguistic Changes\nEven though ChatGPT has only been widely used for 3 years, it has already started leaving its traces in how we speak. Words like delve and meticulous are being used more frequently in academic YouTube talks.\n\n\n\nGPT words in YouTube videos from Yakura et al 2025\n\n\nIt has also been shown that different GPT’s have different writing styles, also known as idiolects.\nSome projects like this one are trying to match GPT ideolects match that of unique users. This will make detecting AI use more difficult in future.\n\n\n\n\n\n\nThis course was written by two people, and each person wrote their own sections without the use of any AI. Can you tell who wrote which sections based on idiolects?\n\n\n\nCan we risk losing our individuality to subtle linguistic shifts?\nWe know that subtle linguistic shifts can change emotional regulation within an individual. We also know that sociolinguistics, even the slightest linguistic features, serve to bind or divide us. At the risk of sounding like an alarmist, we must be vigilant.\n\n\nAI on the Internet\nSocial media platforms are an important aspect in the development, improvement, and implementation of GenAI models. OpenAI, the creators of ChatGPT, have used a subreddit on the social media platform, Reddit, to train their new algorithm. Google and OpenAI have contractual agreements with Reddit to license data from the users on the platform. Earlier in 2025, researchers from the University of Zurich were implicated in an experiment on the same subreddit OpenAI used to train a model. They wanted to test whether an interaction with a bot was more likely to make people change their minds than an interaction with a real person. This was heavily frowned upon, and posts were all removed as users had no ability to consent to participating in a study. It has also been suggested that interactions observed by the researchers were just bots arguing with each other.\nA preprint released in February 2025 by Liang et al found that the amount of content generated by AI rose from 2-3% in November 2022 to 24% by the end of 2023.\n\n\n\nAI slop trough by Yahoo! News!\n\n\n\n\nEnvironmnetal Impact\nThe facilities to run GenAI require a significant amount of resources. These facilities need a huge amount of electricity to power the facility (this places extreme strain on exising infrastructure and increases the grid’s carbon footprint) as well as water to cool the hardware. Currently, data centers use more electricity than many independent countries.\nSome data centers are being built near poor communities, drain resources from, and add pollution to the community (see Colossus that has been built in Memphis to power the X bot, Grok for an example. Musk is not the only offender.)\nWhile we cannot do anything about where data centers are built, we can make informed decisions about which platforms (if any), we use. We can also be careful with the number of queries we send, and how we use our queries. In April 2025, the CEO of OpenAI said that polite requests like “please” and “thank you” have cost tens of millions of dollars due to the cost of electricity.",
    "crumbs": [
      "Home",
      "Background knowledge with a bit of hands-on",
      "Caution: AI in Bioinformatics"
    ]
  },
  {
    "objectID": "Day5Session1_AI.html#how-to-make-the-best-of-the-situation",
    "href": "Day5Session1_AI.html#how-to-make-the-best-of-the-situation",
    "title": "Caution: AI in Bioinformatics",
    "section": "How To Make The Best Of The Situation",
    "text": "How To Make The Best Of The Situation\nIf we know the risks and the true cost of what we are doing, we can make informed decisions about how we chose to incorporate new technologies into our day-to-day and working lives.\nHere are some questions I ask myself before I even open my GPT of choice\n\nAm I phrasing my prompt in a good way? Here is a guide to prompt engineering that might be useful\nCan I find this information any other way?\nHow much time am I saving by looking this question up here vs on BioStars, for example?\nDo I know enough about the topic to know whether the GPT is lying to me?\nWhat are the consequences of testing the validity of the GPT solution? Can I potentially corrupt my data or my system? Am I lie to someone who trusts me enough to ask my opinion?\n\nIf I cannot say with certainty that I will be able to find a hallucination or a lie, I will not put my thoughts to my GPT. If there is even the slightest chance that I am spreading disinformation by repeating information from the GPT, I will not put my thoughts to the GPT.\nIf I use a GPT to learn a new skill, or show a student how to do this, I highlight how important active learning is. I suggest that they seek explanations for everything themselves, and doublecheck everything the GPT tells them.",
    "crumbs": [
      "Home",
      "Background knowledge with a bit of hands-on",
      "Caution: AI in Bioinformatics"
    ]
  },
  {
    "objectID": "Day1Session2_linux.html",
    "href": "Day1Session2_linux.html",
    "title": "A Brief Introduction to Linux",
    "section": "",
    "text": "Unix-like operating systems are built under the model of free and open-source development and distribution. They often come with a graphical user interface (GUI) and can be run from the command line (CLI) or terminal. The CLI is a text-based interface that works exactly the same way as you would use your mouse, but you use words. It can be intimidating at first, but once you have mastered the basics, it’s really not different than using your mouse!\nIt is important to know how to use the terminal as all servers, and most bioinformatics tools do not have a GUI and rely on the use of the terminal.\n\n\n\n\n\n\nWarning\n\n\n\nMacOS and Linux, and Windows have significant differences in their syntax. Where we do things locally, we will point out some of the differences, but for the most part, we will provide Windows users with a local Linux platform so that this course can run as smoothly as possible\n\n\nFor this course, we do not expect you to be masters of Linux, but we will need some knowledge of how to find files, and some other basic Linux commands.",
    "crumbs": [
      "Home",
      "Background knowledge with a bit of hands-on",
      "A Brief Introduction to Linux"
    ]
  },
  {
    "objectID": "Day1Session2_linux.html#introduction",
    "href": "Day1Session2_linux.html#introduction",
    "title": "A Brief Introduction to Linux",
    "section": "",
    "text": "Unix-like operating systems are built under the model of free and open-source development and distribution. They often come with a graphical user interface (GUI) and can be run from the command line (CLI) or terminal. The CLI is a text-based interface that works exactly the same way as you would use your mouse, but you use words. It can be intimidating at first, but once you have mastered the basics, it’s really not different than using your mouse!\nIt is important to know how to use the terminal as all servers, and most bioinformatics tools do not have a GUI and rely on the use of the terminal.\n\n\n\n\n\n\nWarning\n\n\n\nMacOS and Linux, and Windows have significant differences in their syntax. Where we do things locally, we will point out some of the differences, but for the most part, we will provide Windows users with a local Linux platform so that this course can run as smoothly as possible\n\n\nFor this course, we do not expect you to be masters of Linux, but we will need some knowledge of how to find files, and some other basic Linux commands.",
    "crumbs": [
      "Home",
      "Background knowledge with a bit of hands-on",
      "A Brief Introduction to Linux"
    ]
  },
  {
    "objectID": "Day1Session2_linux.html#filesystem-architecture",
    "href": "Day1Session2_linux.html#filesystem-architecture",
    "title": "A Brief Introduction to Linux",
    "section": "Filesystem Architecture",
    "text": "Filesystem Architecture\nLinux uses a hierarchical filesystem, similar to Windows and Mac. In this figure from TecAdmin, there is a representation of this.\n\n\n\nFilesystem\n\n\nWe can see here that the root or / folder is at the top of the hierarchy, with all other folders, like home/ and var/ inside of it.\n\n\n\n\n\n\nNote\n\n\n\n\n\nWe use / at the end of a folder name to show that it is a folder.\n\n\n\nThere are two different ways for us to know where our file is within the operating system. The first is the absolute path and the second is the relative path. The absolute path gives us information that is true anywhere on your operating system. Whether your terminal is open in /usr/bin/something/ or /var/tmp/, a file will always be located at /usr/Documents/sequence.fasta as it is the true or absolute location. The relative path, as the name suggests, is relative to where you currently are on the file tree. If your terminal is open in /usr/Documents/paper/figures/, the file from before, sequence.fasta will be two folders up from where you are. If you were in /var/bin/something/ it would three folder up, one to the side, and two folders down.\n\n\n\n\n\n\nTip\n\n\n\n\n\nIt is often good practice to use absolute paths when you set pipelines up to run- this way, your programs know where to go looking for the files they’re supposed to work on",
    "crumbs": [
      "Home",
      "Background knowledge with a bit of hands-on",
      "A Brief Introduction to Linux"
    ]
  },
  {
    "objectID": "Day1Session2_linux.html#connecting-to-a-server",
    "href": "Day1Session2_linux.html#connecting-to-a-server",
    "title": "A Brief Introduction to Linux",
    "section": "Connecting to a Server",
    "text": "Connecting to a Server\nThere are many different ways you can connect to a server with SSH. At SLUBI, we have a strong preference towards Visual Studio Code or its open-source alternative, VSCodium. It provides a graphical user interface where you can create, edit, and view files, and see the file system.\nTo be able to connect to a server, you need to install an extension called Remote-SSH from the extensions market place.\n\n\n\nExtensions from View\n\n\n\n\n\nExtensions from the shortcut\n\n\nNow we need to add a host to our list of known hosts. There are several ways of doing this, and all ways lead to establishing a connection.\n\nUnder the previously shown View option, navigate to the Command Palette. A dropdown menu at the top will start with a &gt;. Type Remote-SSH and select add new host. Here we will input our credentials. This will populate a file in a hidden folder, .ssh in your local home directories called known_hosts.\nYou can also write all of these lines manually. This way, you can also specify SSH keys that may be needed to log on to particular servers. We won’t be covering that in this course, though.\n\nAfter a host has been added, you can connect to it. You can do this either through the Command Palette, or through the small blue backwards and forwards arrows in the bottom left corner of VSCode. You will receive a list of known hosts that you can connect to. If you need to connect with a password, a password prompt will be shown.\nOnce connected, you can open the file manager on the left.\nFor privacy concerns, we will share the exact commands locally in the room.\n\n\n\n\n\n\nImportant\n\n\n\nRemeber to only open either your home directories or the folder for your projects with the file manager. If you try to open folders that are too large, you will cause VSCode to crash (and you may cause significant problems for your system administrator!)",
    "crumbs": [
      "Home",
      "Background knowledge with a bit of hands-on",
      "A Brief Introduction to Linux"
    ]
  },
  {
    "objectID": "Day1Session2_linux.html#navigating-with-the-terminal",
    "href": "Day1Session2_linux.html#navigating-with-the-terminal",
    "title": "A Brief Introduction to Linux",
    "section": "Navigating with the terminal",
    "text": "Navigating with the terminal\nFor this part of the session, we will do some hands-on practice! For this, we will open a terminal. You can do this through the Command Palette by using the View: Toggle Terminal function, or clicking the box with a dark bottom half and an empty top half in the top right corner of VSCode.\n\n\n\n\n\n\nNote\n\n\n\n\n\nFor Windows Users If you are using a Windows operating system, you can install the free Home edition of MobaXterm when your access to this server is removed if you want to continue practicing your Linux skills.\nTo use MobaXterm as a Linux shell, click on New Session and select a bash shell. There will probably be a prompt that you have to installl an extension. Click on the link it will provide, or copy the extension it needs and search it on Google (or your preferred search engine) and install it. Then try starting a new session again.\n\n\n\nTo see where we currently are in the filesystem, we use the present working directory command. This will give us the absolute path of the directory that our terminal open in.\npwd\nTo see the files that are currently in the directory we use the list command.\nls\nTo make a directory, we use the make directory command, followed by the name we would like our directory to have. Here we will make a directory called sida_training.\nmkdir sida_training\n\n\n\n\n\n\nTip\n\n\n\n\n\nWhen we give files and folders names, we don’t use spaces or special characters. It makes it really difficult to access files. We can use different ways to name our files and folders 1. youcanbechaoticandusenothing 2. YouCanUseCapitalLetters 3. you_can_use_underscores 4. you-can-use-dashes The most important thing is to be consistent with what you use, and to use descriptive names that are not too long. Future you will thank past you!\n\n\n\nTo enter the sida_training directory, we will use the change directory command.\ncd sida_training\nWe can use the pwd command again just to make sure that we are indeed in the right folder.\nTo go back to the folder we were in previously, we can use a shortcut rather than the absolute path.\ncd ..\n\n\n\n\n\n\nTip\n\n\n\n\n\nThere are no limits to the amount of directories you can go back (provided they are in your filesystem). If you wanted to go up 3 directories, you’d use\ncd ../../..\n\n\n\nTo delete files and folder you use the remove command. If you want to remove a folder, you need a recursive flag.\nrm -r sida_training\nIf we had a file to look at, we could use the less command. In the interface, we press q to quit.\nless filename\nWith these Linux basics, you will certainly be able to follow our course. If you would like to learn more, Data Carpentry and Software Carpentry have excellent tutorials on using the terminal in more detail, and we can only recommend them!",
    "crumbs": [
      "Home",
      "Background knowledge with a bit of hands-on",
      "A Brief Introduction to Linux"
    ]
  },
  {
    "objectID": "Day1Session3_pixi.html",
    "href": "Day1Session3_pixi.html",
    "title": "Introduction to Pixi",
    "section": "",
    "text": "Different operating systems bring different challenges to bioinformatics. Windows, for instance, doesn’t really support most bioinformatics tools. One way to solve this through the use of environments. There are, of course, other ways, e.g. running virtual machines. In our experience, environments are a bit easier to manage and are more portable across different systems. There are many different kinds of environments, and for this course, we are going to use Pixi.\n\n\n\nPixi landing page\n\n\nAs you can see, you can run Pixi on all major operating systems, and you can include various platforms in your Pixi environment.\n\n\nInstalling Pixi is really easy and described throughouly here with separate installation guides for Windows and Mac/Linux.\n\n\n\n\n\n\nTip\n\n\n\n\n\nTo source your shell, you need to source the startup files, in Linux it’s the ~/.bashrc file, in Mac it’s the ~/.zshrc file.",
    "crumbs": [
      "Home",
      "Background knowledge with a bit of hands-on",
      "Introduction to Pixi"
    ]
  },
  {
    "objectID": "Day1Session3_pixi.html#introduction",
    "href": "Day1Session3_pixi.html#introduction",
    "title": "Introduction to Pixi",
    "section": "",
    "text": "Different operating systems bring different challenges to bioinformatics. Windows, for instance, doesn’t really support most bioinformatics tools. One way to solve this through the use of environments. There are, of course, other ways, e.g. running virtual machines. In our experience, environments are a bit easier to manage and are more portable across different systems. There are many different kinds of environments, and for this course, we are going to use Pixi.\n\n\n\nPixi landing page\n\n\nAs you can see, you can run Pixi on all major operating systems, and you can include various platforms in your Pixi environment.\n\n\nInstalling Pixi is really easy and described throughouly here with separate installation guides for Windows and Mac/Linux.\n\n\n\n\n\n\nTip\n\n\n\n\n\nTo source your shell, you need to source the startup files, in Linux it’s the ~/.bashrc file, in Mac it’s the ~/.zshrc file.",
    "crumbs": [
      "Home",
      "Background knowledge with a bit of hands-on",
      "Introduction to Pixi"
    ]
  },
  {
    "objectID": "Day1Session3_pixi.html#setting-up-an-environment",
    "href": "Day1Session3_pixi.html#setting-up-an-environment",
    "title": "Introduction to Pixi",
    "section": "Setting Up An Environment",
    "text": "Setting Up An Environment\nYou should create separate environments for each project you run, just to keep things tidy. To create an environment, you have to specify a name for your environment. You can include different platforms/operating systems in your environment, as well as different channels.\nHere, we will create a project called name_sida_training (please use your own name to avoid). We are adding the conda-forge and bioconda channels with the -c flag.\npixi init name_sida_training -c conda-forge -c bioconda\n\n\n\n\n\n\nNote\n\n\n\nIf you want to add different platforms, you add them with the -p linux-64 flag. In this example, you are adding Linux64. See the Pixi docs for the full list of supported platforms.\nIf you are adding a platform that doesn’t natively run on your OS (e.g. adding Linux when running on Windows) be sure to add the OS you are running your system on as well!\n\n\nWhen you change directories into the folder pixi created you will see two files: pixi.lock and pixi.toml.\n\n\n\n\n\n\nTip\n\n\n\n\n\nHere is the code for changing directories, listing files, and viewing the contents of a file:\ncd name_sida_training\nls\nless pixi.toml\nTo exit the less view, press q for quit.\n\n\n\n\npixi.toml\nThe .toml file give your information about your project. Let’s have a look at one I made one my Mac before adding any dependencies to my environment. How is it different from the one we’ve made on HPC2N?\n[workspace]\nchannels = [\"conda-forge\", \"bioconda\"]\nname = \"sida_quarto\"\nplatforms = [\"osx-arm64\"]\nversion = \"0.1.0\"\n\n[tasks]\n\n[dependencies]\n \n\n\npixi.lock\nThe .lock file give you information on the channels you have decided to add, as well as the information on where the packages were downloaded from, license information, md5 information, and more.\n\n\n\n\n\n\nImportant\n\n\n\nDo not delete the .toml or .lock files, or you will break your environment!",
    "crumbs": [
      "Home",
      "Background knowledge with a bit of hands-on",
      "Introduction to Pixi"
    ]
  },
  {
    "objectID": "Day1Session3_pixi.html#adding-dependencies",
    "href": "Day1Session3_pixi.html#adding-dependencies",
    "title": "Introduction to Pixi",
    "section": "Adding Dependencies",
    "text": "Adding Dependencies\nAdding dependencies is like installing a program. Instead of installing it globally, it only gets installed in the environment.\nTo do this, we use the pixi add function. Let’s try adding Quarto to our environments (we will use Quarto extensively in this course!)\n\n\n\n\n\n\nImportant\n\n\n\nYou must be in the folder of the project to add software!\n\n\npixi add quarto\n\n\n\n\n\n\nTip\n\n\n\n\n\nIf you are unsure of how a function works, you can always query it, usually with the --help or -h flags. Here’s how it would look for the pixi add function\npixi add --help\nA general rule of thumb is that a single hyphen - is followed by a single letter flag, while double hyphens -- are usually followed by multi-letter flags\n\n\n\nHere is the pixi.toml from before after I have added Quarto to my environment. You can see that the dependencies have been updated to include Quarto.\n[workspace]\nchannels = [\"conda-forge\", \"bioconda\"]\nname = \"sida_test\"\nplatforms = [\"osx-arm64\"]\nversion = \"0.1.0\"\n\n[tasks]\n\n[dependencies]\nquarto = \"&gt;=1.7.32,&lt;2\"",
    "crumbs": [
      "Home",
      "Background knowledge with a bit of hands-on",
      "Introduction to Pixi"
    ]
  },
  {
    "objectID": "Day1Session3_pixi.html#running-a-package",
    "href": "Day1Session3_pixi.html#running-a-package",
    "title": "Introduction to Pixi",
    "section": "Running a Package",
    "text": "Running a Package\nNow that we have an environment with a package running in it, we actually want to use it. Let’s query the help function within Quarto.\npixi run quarto --help\n\n\n\n\n\n\nNote\n\n\n\nTo use other tools, simply substitute quarto with the package you’ve added",
    "crumbs": [
      "Home",
      "Background knowledge with a bit of hands-on",
      "Introduction to Pixi"
    ]
  },
  {
    "objectID": "Day3Session2_quarto.html",
    "href": "Day3Session2_quarto.html",
    "title": "Introduction to Markdown and Quarto",
    "section": "",
    "text": "Markdown is a lightweight markup language for text editing. All of the documents you have seen in this course have been formatted with markdown. The platform this course website has been built on is called Quarto and implements markdown. In this section, we’ll look at some of the post commonly used markdown syntax, using Quarto to create lab notebooks, presentations, automate analyses, and host websites like this one.\nRecently, a lot of open-source books like R for Data Science and Python for Data Science have been created with Markdown using Quarto. The code for these books have also been publicly released.\n\n\n\n\n\n\nImportant\n\n\n\nPlease bookmark these books for your students! They are excellent resources!",
    "crumbs": [
      "Home",
      "Data management",
      "Introduction to Markdown and Quarto"
    ]
  },
  {
    "objectID": "Day3Session2_quarto.html#introduction",
    "href": "Day3Session2_quarto.html#introduction",
    "title": "Introduction to Markdown and Quarto",
    "section": "",
    "text": "Markdown is a lightweight markup language for text editing. All of the documents you have seen in this course have been formatted with markdown. The platform this course website has been built on is called Quarto and implements markdown. In this section, we’ll look at some of the post commonly used markdown syntax, using Quarto to create lab notebooks, presentations, automate analyses, and host websites like this one.\nRecently, a lot of open-source books like R for Data Science and Python for Data Science have been created with Markdown using Quarto. The code for these books have also been publicly released.\n\n\n\n\n\n\nImportant\n\n\n\nPlease bookmark these books for your students! They are excellent resources!",
    "crumbs": [
      "Home",
      "Data management",
      "Introduction to Markdown and Quarto"
    ]
  },
  {
    "objectID": "Day3Session2_quarto.html#markdown-syntax",
    "href": "Day3Session2_quarto.html#markdown-syntax",
    "title": "Introduction to Markdown and Quarto",
    "section": "Markdown Syntax",
    "text": "Markdown Syntax\nThere are plenty of really great cheatsheets like this one from Markdown Guide as well as this one. We’ll go through some of these details with you live rather than copying and pasting really good tutorials here.",
    "crumbs": [
      "Home",
      "Data management",
      "Introduction to Markdown and Quarto"
    ]
  },
  {
    "objectID": "Day3Session2_quarto.html#using-markdown",
    "href": "Day3Session2_quarto.html#using-markdown",
    "title": "Introduction to Markdown and Quarto",
    "section": "Using Markdown",
    "text": "Using Markdown\nThere are many different platforms that use Markdown syntax in text editing. At SLUBI, we have a strong preference towards Visual Studio Code or its open-source alternative, VSCodium. To use all of the features, you need to install the Markdown extension.\nTo access to Extensions, either navigate with shortcuts (these are operating system depdenent and can be set by users, so we won’t go into that, but feel free to explore), from the View dropdown menu, or from the Extensions tab on the left.\n\n\n\nExtensions from View\n\n\n\n\n\nExtensions from the shortcut\n\n\nIn the search bar, you can search for general Markdown viewers and install them. For this course, we will use Quarto. Install that extension. Please also note the verified tick.\n\n\n\nQuarto extension to install\n\n\nThe Markdown All in One extension is also an interesting case to look at. It has a lot of downloads despite not being verified by an organisation. In general, be wary when installing extensions. Opt for ones that are installed often, as this one is. If possible, try to use verified extensions. In the age of information security we live in, it is becoming more important to use trusted content than whatever you find on the internet.",
    "crumbs": [
      "Home",
      "Data management",
      "Introduction to Markdown and Quarto"
    ]
  },
  {
    "objectID": "Day3Session2_quarto.html#quarto",
    "href": "Day3Session2_quarto.html#quarto",
    "title": "Introduction to Markdown and Quarto",
    "section": "Quarto",
    "text": "Quarto\nQuarto is an open-source technical and scientific reporting and publishing system.\n\n\n\nQuarto landing page\n\n\n\nInstalling Quarto on your Machine\nWe have already installed the extension that allows us to use a bunch of really cool features in VSCode with Quarto, but we need to have the backend installed on our machines as well. To install the program, you will click on the Get Started link on the homepage, download the program that’s suited to your operating system, and install it. The Guide link here also takes you to really easy tutorials on how to set up a variety of other uses of Quarto that we simply don’t have time for in this course.\n\n\n\n\n\n\nImportant\n\n\n\nAll instructions for creating, editing, previewing, and rendering projects will be shown in VSCode and it will not be specified over and over\n\n\n\n\nCreating a Project\nUnder the previously shown View option, navigate to the Command Palette. A dropdown menu at the top will start with a &gt;. Type quarto and a list of functions will be shown. The order will be different based on what you’ve used and what you’ve used most recently.\n\n\n\nQuarto functions\n\n\nWe will Create a project and create a Website for the purpose of this course.\n\n\n\nCreating a Project\n\n\n\nStructure of a Project\nAll Quarto projects have a YAML file with information about the project called _quarto.yml as well as an index.qmd file. The landing page of your website is the index.qmd file, while the _quarto.yml file contains all of the visual aspects of your website. We’ll spend a bit of time looking at the design of our course website so that you can see how it looks behind the scenes. Hopefully, with a website you know as well as you do by now, this will make things a lot more tangible!\n\n\n\n\n\n\nNote\n\n\n\n\n\nFun fact: YAML stands for “yet another markup language”\n\n\n\n\n\n\nAdding Files and Figures\nOften, we don’t just need a landing page like the index.qmd file, but we want different pages, as we have here. You can add blank documents with the .qmd extenstion. It is important for each file to have a title section so that it can be displayed properly and linked with your other pages. Update the _quarto.yml file to include this page in your table of contents or navigation bar (depending on what you opted to use), or simply link to the file with a hyperlink as we have done on our landing page\n\n\n\n\n\n\nTip\n\n\n\nRemeber to give your files a good name with no spaces in them. Also remember to add the .qmd extension\n\n\nTo add external figures to your document as we have done here with screenshots, you can simply link to the figure by its relative path, as described here.\n\n\n\n\n\n\nTip\n\n\n\nIt is good practice to have your images in a folder inside of your Quarto project in case Future You forgets that a figure is linked to a document and you move the folder containing all of your photos, for example. It also makes it much easier to put everything into a repository on GitHub to launch your site\n\n\n\n\nAdding Analyses\nQuarto is really good at implementing other programming languages within it. You can use R within it seamlessly. There is support for a large number of other programming languages within Quarto, but this course is only going to showcase the use of R.\n\n\n\n\n\n\nImportant\n\n\n\nYou will have to install the R and R syntax extensions for this to work in VSCode\n\n\n\npacman::p_load(tidyverse, palmerpenguins)\n\nggplot(\n  data = penguins,\n  mapping = aes(x = flipper_length_mm, y = body_mass_g)\n) +\n  geom_point(mapping = aes(color = species, shape = species)) +\n  geom_smooth(method = \"lm\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nIf you have analyses that you have to repeat across several projects, these documents can be shared to create uniformity among your produced results, and your students do not need to reinvent the wheel. With several pages being supported in a single Quarto project, you also reduce the potential messiness that is introduced whenever you onboard a new student.\n\n\nViewing the Project\n\nPreviewing\nAs you go along, you might be interested in seeing what your pages look like. In the terminal, type\nquarto preview\nor simply click the preview button in VSCode and a side by side preview will open where you can click through your indexed pages.\n\n\n\nVSCode Preview function\n\n\n\n\nDifferent Views\nFrom the Command Palette, you can select whether you would like to view your document in source mode or in visual mode. The difference is mostly that you can interact with the document in the way it will be rendered in visual mode. For some people, visual mode is a lot more intuative, and there is no right or wrong way to interact with your documents.\n\n\n\nPublishing and Sharing the Project\nWith Quarto, you can render to HTML, Word, and PDF formats. To render individual files into PDF, add the information from this Quarto guide into the header of your document.\n---\ntitle: \"My document\"\nformat:\n  pdf:\n    toc: true\n    number-sections: true\n    colorlinks: true\n---\nYou will have to install a recent distribution of TeX for this via your terminal\nquarto install tinytex\nFor this course, we are simply going to render our documents into HTML format. In the next section, we will cover how to host Quarto pages on GitHub.",
    "crumbs": [
      "Home",
      "Data management",
      "Introduction to Markdown and Quarto"
    ]
  },
  {
    "objectID": "Day2Session1_containers.html",
    "href": "Day2Session1_containers.html",
    "title": "Containers",
    "section": "",
    "text": "There are several underlying problems within bioinformatics (and informatics in general).\n\nIt is not uncommon for people within the same team to use different operating systems (whether MacOS, Windows, or different flavours of Unix builds). Even if everyone is using a MacOS, there are still different versions that impact the way people are able to work with their machines.\nAlmost every piece of software has some sort of dependency it needs to run. Some programs might “just” need a bash shell or basic python, while others need a variety of compilers and additional libraries to function. Often, these dependencies require further dependencies to be installed. It is also not uncommon for dependencies for Program 1 to clash with the dependencies for Program 2, requiring the user to uninstall dependencies to be able to install other dependencies.\nIn bioinformatics, tools are very often not maintained after the student that wrote the software graduated, the PI moved to a different university, or the funding simply ran out. This leads to a lot of really good software not really being supported by newer operating systems, usually due to dependencies not being easily available or, as before, clashing with newer versions. This makes installing a tool one of the biggest hurdles to overcome in bioinformatics.\nYou often cannot install different versions of the same program on one computer due to conflicting names. This is particularly problematic when you want to rerun an analysis for a publication where you need to use the same software all the way through.\n\nAll of these problems mean that bioinformatics becomes less reproducible as they cannot be moved easily betweem systems (either when you upgrade your computer or want to share your pipeline with a colleague). Most of these problems can be overcome with containers.",
    "crumbs": [
      "Home",
      "Background knowledge with a bit of hands-on",
      "Containers"
    ]
  },
  {
    "objectID": "Day2Session1_containers.html#problems",
    "href": "Day2Session1_containers.html#problems",
    "title": "Containers",
    "section": "",
    "text": "There are several underlying problems within bioinformatics (and informatics in general).\n\nIt is not uncommon for people within the same team to use different operating systems (whether MacOS, Windows, or different flavours of Unix builds). Even if everyone is using a MacOS, there are still different versions that impact the way people are able to work with their machines.\nAlmost every piece of software has some sort of dependency it needs to run. Some programs might “just” need a bash shell or basic python, while others need a variety of compilers and additional libraries to function. Often, these dependencies require further dependencies to be installed. It is also not uncommon for dependencies for Program 1 to clash with the dependencies for Program 2, requiring the user to uninstall dependencies to be able to install other dependencies.\nIn bioinformatics, tools are very often not maintained after the student that wrote the software graduated, the PI moved to a different university, or the funding simply ran out. This leads to a lot of really good software not really being supported by newer operating systems, usually due to dependencies not being easily available or, as before, clashing with newer versions. This makes installing a tool one of the biggest hurdles to overcome in bioinformatics.\nYou often cannot install different versions of the same program on one computer due to conflicting names. This is particularly problematic when you want to rerun an analysis for a publication where you need to use the same software all the way through.\n\nAll of these problems mean that bioinformatics becomes less reproducible as they cannot be moved easily betweem systems (either when you upgrade your computer or want to share your pipeline with a colleague). Most of these problems can be overcome with containers.",
    "crumbs": [
      "Home",
      "Background knowledge with a bit of hands-on",
      "Containers"
    ]
  },
  {
    "objectID": "Day2Session1_containers.html#containers",
    "href": "Day2Session1_containers.html#containers",
    "title": "Containers",
    "section": "Containers",
    "text": "Containers\n\nWhat are containers?\nContainers are stand-alone pieces of software that require a container management tool to run. Containers contain an operating system, all dependencies, and software in an isolates environment. Container management tools can be run on all operating systems, and since the container has the operating system within it, it will run the same in all environments. Containers are also immutable, so it is stable over time.\n\n\nRunning Containers\nThere are several programs that can be used to run containers. Docker, Appptainer, and Podman are the most commonly used platforms. They all have their pros and cons. If you are using a Windows machine that only you are using, then Docker is likely the least complex tool to install. On multi-user systems like a server, Apptainer is the best tool for the job. For this tutorial and the rest of the course, we will use Apptainer commands. There are small syntax changes between bash and powershell commands, but they are very similar.\n\n\nDownloading Containers\nThere are several repositories for people to upload containers that they have built. dockerhub and Seqera are two commonly used platforms for downloading containers. You are able to run containers from dockerhub on Apptainer without any problems.\n\ndockerhub Tutorial\nOn the dockerhub landing page, you have a search bar, and some login options. You do not need to create an account to access the containers on dockerhub.\n\n\n\ndockerhub landing page\n\n\nFor these tutorials, we’ll search VCFtools, a commonly used software for VCF manipulation and querying. The results of the search give us several different containers with the same name.\n\n\n\nRegistry search\n\n\nYou can see who built the container, how many times it has been pulled, when it was updated (here updated means different versions of the container being uploaded), and how many people have starred it. It is usually a good rule of thumb to use the most popular containers from users that have uploaded a lot of containers. The biocontainers and pegi3s profiles have builds for a lot of tools, and they are built really well!\nIf we click on the image from biocontainers we get to a typical dockerhub image landing page:\n\n\n\nVCFtools page\n\n\nThere is information on the frequency of the container being pulled, as well as a pull command. This command is for docker, so we need to modify it for Apptainer.\napptainer pull vcftools_0.1.16-1.sif docker://biocontainers/vcftools\nThis command has several parts to it:\n\napptainer calls on the Apptainer software to run\npull tells Apptainer which function to use. In this case, we want it to go fetch something from a repository\nvcftools_0.1.16-1.sif is the name of the container on our local machine. We could call it I_Love_Dogs but that is not very informative at all. Your collaborator won’t know what it means, and you certainly won’t know what it means in 6 months from now! It is also good practice to put the version number in your image name in case you want to have several versions at the same time, and you need to tell them apart.\ndocker:// is the registry you are pulling from. There are several different registreis, but we are only going to show 2 during this course. (You will see another one in the Seqera tutorial)\nbiocontainers/vcftools is the profile/repository and container you are pulling\n\n\n\n\n\n\n\nTip\n\n\n\n\n\nFile format extensions like .txt and .sif are really only important for us. However, it is good practice to append your files with appropriate extensions to ensure that you follow good data management practices\n\n\n\nIf you are interested in a different version than the current version, there are other versions under the tags tab:\n\n\n\nContainer versions\n\n\nIf you wanted to download another version of the container, you simply copy the command shown on the right side, and alter the syntax, for example\napptainer pull vcftools0.1.14.sif docker://biocontainers/vcftools:v0.1.14_cv2\n\n\nSeqera Tutorial\nThe Seqera landing page is a bit different from the dockerhub landing page, and it works a bit differently from dockerhub. dockerhub hosts container images that users have uploaded, while Seqera builds containers as you request them. They use bioconda, conda-forge, and pypi libraries to build their containers with Wave. The advantage is that you can include several different softwares in your container image at once. The disadvantage is that you are limited to software hosted on the aforementioned repositories. Usually this isn’t a problem, but sometimes you want to use something that isn’t hosted there.\n\n\n\nSeqera containers landing page\n\n\nWhen you pull an image from Seqera and want to run it with Apptainer, you need to remember to change the container setting from Docker to Singularity\n\n\n\nSelecting Singularity\n\n\nSince Seqera builds containers on-demand, sometimes you have to wait for the container to finish compiling. You can see that it is still building the container from the fetching container comment. Don’t try to pull it when it is still building!\n\n\n\nWaiting to build\n\n\nWhen the container is built, you can copy the text and pull the container to your system\n\n\n\nReady to download\n\n\napptainer pull vcftools_0.1.17.sif oras://community.wave.seqera.io/library/vcftools:0.1.17--b541aa8d9f9213f9\nHere we use oras:// instead of docker:// as we are pulling from the oras registry. We are also pulling a different version from Seqera, so the name of the container is different.\n\n\n\nRunning Containers\nOnce you have the container on your local machine, you want to be able to use it. Apptainer can be used to enter the container and run as if you had the exact same operating system as the person who built it, or it can run the software inside the container from outside of the container.\nThere are 2 different ways to use a container: run and exec. The apptainer run command launches the container and first runs the %runscript for the container if one is defined, and then runs your command (we will cover %runscript in the Building Containers section). The apptainer exec command will not run the %runscript even if one is defined. It is a small, fiddly detail that might be applicable if you use other people’s containers. After calling Apptainer and the run or exec commands, you can use your software as you usually would\napptainer exec vcftools_0.1.17.sif vcftools --version\nThis command runs your vcftools_0.1.17.sif container, calls on the program vcftools that is within the container, and will show you the version. If you had installed VCFtools locally, you would have just used\nvcftools --version\n\n\n\n\n\n\nImportant\n\n\n\nPlease remember that VCFtools is just an example. If you want to run any other tool everything after apptainer run or apptainer exec has to substitute the name of your container and the run commands for that particular tool!\n\n\n\n\nBuilding Containers\nIf the software you would like to use is not packaged into a container by anyone else, you might want to build it yourself. For this, we are just going to show a very simple example. Building containers from scratch is a computationally intensive task. You build containers from a definition file with the extension .def\nHere we are going to build a container with a cow telling us the date. Save this in a file called lolcow.def\nBootstrap: docker\nFrom: ubuntu:20.04\n\n%post\n    apt-get -y update\n    apt-get -y install cowsay lolcat fortune\n\n%environment\n    export LC_ALL=C\n    export PATH=/usr/games:$PATH\n\n%runscript\n    date | cowsay | lolcat    \nThere are several components to this definition file.\n\nYou can set the operating system you want in the container, in this case Ubuntu 20.04\n%post section is where you update the OS from its base state, install dependencies and so on\n%environment is where you export paths and modify the environment\n%runscript is the script that will run when you use apptainer run container.sif. If you don’t include a runscript, then nothing will happen when you try to run it without any commands. You could build this container without anything in the %runscript section, and use apptainer run container.sif date | cowsay | lolcat to get the same output.\n\napptainer build lolcow.sif lolcow.def\nYou’ll get a lot of output on the status of the build, ending of\nINFO:    Adding environment to container\nINFO:    Adding runscript\nINFO:    Creating SIF file...\nINFO:    Build complete: lolcow.sif\nWe can now run our new container with\napptainer run lolcow.sif\n\n\n\nBoring cow\n\n\n\n\n\n\n\n\nNote\n\n\n\nTry removing the %runscript, build it again, and see what happens\n\n\nBootstrap: docker\nFrom: ubuntu:20.04\n\n%post\n    apt-get -y update\n    apt-get -y install cowsay lolcat fortune\n\n%environment\n    export LC_ALL=C\n    export PATH=/usr/games:$PATH\n\n%runscript\n    fortune | cowsay | lolcat    \nIf we use the same definition file as before, but substitute date for fortune in the runscript and build the container, we now get a philosophical cow with a dark sense of humour\n\n\n\nFun cow\n\n\n\n\n\nInspirational cow\n\n\nTo show the difference between the run and exec commands, we can use the same container with fortune in the runscript and run\napptainer run lolcow.sif date|cowsay\nand\napptainer exec lolcow.sif date|cowsay\nThe run command gives us a philosophical cow while exec gives us our boring cow again",
    "crumbs": [
      "Home",
      "Background knowledge with a bit of hands-on",
      "Containers"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This course is a hosted by the Swedish Agricultural University’s Bioinformatics Infrastructure (SLUBI) team. In this course we hope to give you information on how to use reproducible bioinformatics pipelines, report results in a streamlined manner, and implement the system in your own research groups.\nThis course is funded by SIDA and is hosted in Alnarp, Sweden on the 25th-29th August 2025"
  },
  {
    "objectID": "Day4Session2_use_containers.html",
    "href": "Day4Session2_use_containers.html",
    "title": "Using Containers",
    "section": "",
    "text": "The aim of this session is to provide you with support if you would like to try to download and run your own containers. If there are common points we missed in our introductory session, or simply common questions, we will add these here.",
    "crumbs": [
      "Home",
      "Hands-on training",
      "Using Containers"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "SLUBI SIDA Course",
    "section": "",
    "text": "We are pleased to have this course with you! Our main aim with this course is to share how we use bioinformatics tools in a reproducible and scalable way. We will showcase the use of environments, containers, and established pipelines so that you can run these analyses on any operating system, as well as on systems that are not high performance computing clusters.\nOur approach to this course is going to be more practical and outcome based than theory based. We would like to give you the practical experience and skills to implement these kinds of analyses at your home universities. Our timeplan is also fairly flexible so that we can spend time with you on things that are useful and important for you to take hard skills with you. We can always go into more detail of something if you would like to know more about something.\nThe website will remain active after the course so that you have access to the material, and we encourage you to share these resources with your students and other researchers that might benefit from this!\n\nProgram\n\n\nDay\nSession\n\n\n\n\nMonday\nIntroduction to Linux\n\n\n\nIntroduction to Environments\n\n\n\nIntroduction to nf-core and Nextflow\n\n\nTuesday\nIntroduction to Containers\n\n\n\nSetting up and running a Nextflow Pipeline (with us)\n\n\n\nSetting up and running a Nextflow Pipeline (by yourself)\n\n\nWednesday\nData Management and Reproducible Research\n\n\n\nGitHub\n\n\n\nIntroduction to Markdown and Quarto\n\n\n\nHow do Nextflow Pipelines Work?\n\n\nThursday\nNextflow Results\n\n\n\nUsing Containers Outside of Nextflow\n\n\n\nUsing R and Other Languages in Quarto\n\n\nFriday\nAI in Bioinformatics\n\n\n\nWhat are your needs? How can you implement this in your own institutions?\n\n\n\nFriday is an emptier day in case we run out of time for something, or if there is something that you would like to learn more about.\nEvery day will end with a feedback session, and we hope that you will tell us what you like, what isn’t working for you, and what you would like to see more of.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "Day2Session3_nextfow_own_choice.html",
    "href": "Day2Session3_nextfow_own_choice.html",
    "title": "Setting up a different nfcore pipeline",
    "section": "",
    "text": "For this section, we would like you to test out a different pipeline from nf-core that might be appropriate for your research. We are here to support you in this session.",
    "crumbs": [
      "Home",
      "Hands-on training",
      "Setting up a different nfcore pipeline"
    ]
  },
  {
    "objectID": "Day2Session3_nextfow_own_choice.html#things-to-think-about",
    "href": "Day2Session3_nextfow_own_choice.html#things-to-think-about",
    "title": "Setting up a different nfcore pipeline",
    "section": "Things to think about",
    "text": "Things to think about\n\nIs the pipeline maintained?\nWhat do you need as input?\nWhat are some of the constraints you might face during the setup?",
    "crumbs": [
      "Home",
      "Hands-on training",
      "Setting up a different nfcore pipeline"
    ]
  },
  {
    "objectID": "Day3Session4_nfcore_main.html",
    "href": "Day3Session4_nfcore_main.html",
    "title": "Examining main.nf",
    "section": "",
    "text": "In this session, we will go through the main.nf file that actually executes the whole pipeline of the rnaseq pipeline we ran on Tuesday. We hope that this session will make the whole process less of a “black box” and more approachable.\nYou can find the main.nf file on GitHub\nOne of us will act as a scribe during the session, and record our most important discussion points here for you to look at later.",
    "crumbs": [
      "Home",
      "Hands-on training",
      "Examining main.nf"
    ]
  }
]